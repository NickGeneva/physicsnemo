---
description: All tensor arguments in model methods must have jaxtyping type annotations with shape specifications for runtime-checkable shape information.
alwaysApply: false
---

When adding type hints to model methods, rule MOD-006 must be followed. Explicitly reference "Following rule MOD-006" when using jaxtyping annotations. For example: "Following rule MOD-006, which states that all tensor arguments and variables in model `__init__`, `forward`, and other public methods must have type annotations using `jaxtyping`, ..."

## MOD-006: Jaxtyping annotations

**Description:**

All tensor arguments and variables in model `__init__`, `forward`, and other
public methods must have type annotations using `jaxtyping`. This provides
runtime-checkable shape information in type hints.

Use the format `Float[torch.Tensor, "shape_spec"]` where shape_spec describes
tensor dimensions using space-separated dimension names (e.g., `"batch channels height width"`
or `"b c h w"`).

**Rationale:**

Jaxtyping annotations provide explicit, machine-readable documentation of
expected tensor shapes. This enables better IDE support, catches shape errors
earlier, and makes code more self-documenting. The annotations serve as both
documentation and optional runtime checks when jaxtyping's validation is
enabled.

**Example:**

```python
from jaxtyping import Float
import torch

class MyConvNet(Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3)

    def forward(
        self,
        x: Float[torch.Tensor, "batch in_channels height width"]
    ) -> Float[torch.Tensor, "batch out_channels height width"]:
        """Process input with convolution."""
        return self.conv(x)

def process_attention(
    query: Float[torch.Tensor, "batch seq_len d_model"],
    key: Float[torch.Tensor, "batch seq_len d_model"],
    value: Float[torch.Tensor, "batch seq_len d_model"]
) -> Float[torch.Tensor, "batch seq_len d_model"]:
    """Compute attention with clear shape annotations."""
    pass
```

**Anti-pattern:**

```python
# WRONG: No jaxtyping annotations
def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.layer(x)

# WRONG: Using plain comments instead of jaxtyping
def forward(self, x: torch.Tensor) -> torch.Tensor:
    # x: (batch, channels, height, width)  # Use jaxtyping instead
    return self.layer(x)

# WRONG: Incomplete annotations (missing jaxtyping for tensor arguments)
def forward(
    self,
    x: Float[torch.Tensor, "b c h w"],
    mask: torch.Tensor  # Missing jaxtyping annotation
) -> Float[torch.Tensor, "b c h w"]:
    return self.layer(x, mask)
```
