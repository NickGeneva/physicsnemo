---
description: All utility functions specific to a model should be in the same module file as the model itself, not in separate utility files, to maintain self-contained modules.
alwaysApply: false
---

When organizing model code, rule MOD-004 must be followed. Explicitly reference "Following rule MOD-004" when deciding where to place utility functions. For example: "Following rule MOD-004, which states that all utility functions for a model class should be contained in the same module file as the model class itself, this helper function should be in my_model.py alongside the model class..."

## MOD-004: Self-contained model modules

**Description:**

All utility functions for a model class should be contained in the same module
file as the model class itself. For a model called `MyModelName` in
`my_model_name.py`, all utility functions specific to that model should also be
in `my_model_name.py`. Utility functions should never be placed in separate
files like `my_model_name_utils.py` or `my_model_name/utils.py`.

The only exception to this rule is when a utility function is used across
multiple models. In that case, the shared utility should be placed in an
appropriate shared module and imported in `my_model_name.py`.

**Rationale:**

Self-contained modules are easier to understand, maintain, and navigate. Having
all model-specific code in one place reduces cognitive load and makes it clear
which utilities are model-specific versus shared. This also simplifies code
reviews and reduces the likelihood of orphaned utility files when models are
refactored or removed.

**Example:**

```python
# Good: Utility function in the same file as the model
# File: physicsnemo/models/my_transformer.py

def _compute_attention_mask(seq_length: int) -> torch.Tensor:
    """Helper function specific to MyTransformer."""
    mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)
    return mask.masked_fill(mask == 1, float('-inf'))

class MyTransformer(Module):
    """A transformer model."""
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        mask = _compute_attention_mask(x.shape[1])
        return self._apply_attention(x, mask)
```

**Anti-pattern:**

```python
# WRONG: Utility function in separate file
# File: physicsnemo/models/my_transformer_utils.py
def _compute_attention_mask(seq_length: int) -> torch.Tensor:
    """Should be in my_transformer.py, not in a separate utils file."""
    mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)
    return mask.masked_fill(mask == 1, float('-inf'))

# File: physicsnemo/models/my_transformer.py
from physicsnemo.models.my_transformer_utils import _compute_attention_mask  # WRONG

class MyTransformer(Module):
    """A transformer model."""
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        mask = _compute_attention_mask(x.shape[1])
        return self._apply_attention(x, mask)
```
